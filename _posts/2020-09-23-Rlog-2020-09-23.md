---
layout: post
tag: rlog
---

### Todo

Chap 4 of the Bandit Algorithm (BA) book

Chap 2 of the BA book (in parallel)

### Done

Chap 1 of the BA book.

Sec 4.1 and 4.2 and a bit of 4.3


### Thoughts

1) The competitor class is similar to the class of experts in online learning. The regret is also similar.

Core question: how the regret $R_n$ grows as the number of round $n$ grows. Goal: $o(n)$.

2) Suprised to see the definition of adversarial bandits: "drop all assumptions on how the rewards are generated,
except that they are chosen without knowledge of the learnerâ€™s actions and lie in a bounded set". Doesn't seem to be very adversarial. 

3) Any real websites using bandit algos for A/B testing?

4) Got the first and the last reason for why maximizing $S_n$ is not an optimization problem.

5) First time seeing the categorization of structured and unstructed bandits. Interesting, RL doesn't have this. In RL there's model-free vs model-based.